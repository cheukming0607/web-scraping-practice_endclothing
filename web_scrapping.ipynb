{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-scrapping on endclothing.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Store the root url of the endclothing.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root url of the website for scrapping\n",
    "root_url = \"https://www.endclothing.com/ca/clothing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Get the department and color category from the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departments: ['Blazers', 'Bomber Jackets', 'Boxer Shorts', 'Cardigans', 'Cargo Pants', 'Casual Jackets', 'Casual Trousers', 'Chinos', 'Coach Jackets', 'Cycling Shorts', 'Denim Jackets', 'Dressing Gowns', 'Dungarees', 'Fleeces', 'Gilets', 'Hoodies', 'Jackets & Coats', 'Jeans', 'Knitted Vests', 'Knitwear', 'Leather Jackets', 'Long Coats', 'Long Sleeve Lounge Tops', 'Long Sleeve Polos', 'Long Sleeve Tees', 'Long Sleeve Tops', 'Lounge Pants', 'Lounge Shorts', 'Lounge Tops', 'Loungewear', 'Parkas', 'Polos', 'Puffer Jackets', 'Shirt Jackets', 'Shirts', 'Short Sleeve Shirts', 'Short Sleeve Sweats', 'Short Sleeve T-Shirts', 'Shorts', 'Suits', 'Sweat Pants', 'Sweaters', 'Sweats', 'Swimwear', 'T-Shirts', 'Tailored Trousers', 'Technical & Rain Jackets', 'Tops', 'Track Pants', 'Track Tops', 'Trench & Long Coats', 'Trousers', 'Underwear', 'Vests', 'Waistcoats']\n",
      "No. of department: 55\n",
      "Colors: ['Black', 'Blue', 'Brown', 'Burgundy', 'Gold', 'Green', 'Grey', 'Multi', 'N/A', 'Navy', 'Neutrals', 'Orange', 'Pink', 'Purple', 'Red', 'Silver', 'Thyme', 'White', 'Yellow']\n",
      "No. of Colors: 19\n"
     ]
    }
   ],
   "source": [
    "# Web-scrapping the dept and colors of the clothes and store them into two arrays\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.ChromiumEdge()       # Using Edge as our web scrapping browser\n",
    "driver.get(root_url)\n",
    "\n",
    "depts_list = []\n",
    "colors_list = []\n",
    "depts = driver.find_elements(By.XPATH, '//div[@data-test-id=\"department_FilterItem\"]')      # Using XPATH to point to the department items\n",
    "for i in range(len(depts)):\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", depts[i])       # Use JavaScript syntax to scroll the element to render the text\n",
    "    depts_list.append(depts[i].text)\n",
    "\n",
    "\n",
    "colors = driver.find_elements(By.XPATH, '//div[@data-test-id=\"colour_FilterItem\"]')     # Using XPATH to point to the color items\n",
    "for i in range(len(colors)):\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", colors[i])      # Use JavaScript syntax to scroll the element to render the text\n",
    "    colors_list.append(colors[i].text)\n",
    "\n",
    "print(F\"Departments: {depts_list}\")\n",
    "print(F\"No. of department: {len(depts_list)}\")\n",
    "print(F\"Colors: {colors_list}\")\n",
    "print(F\"No. of Colors: {len(colors_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Build a class object to organize better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a class \"ProductCard\" to store the information of each product\n",
    "class ProductCard:\n",
    "    def __init__(self, name, color, color_cat, price, dept) -> None:\n",
    "        self._name = name\n",
    "        self._color = color\n",
    "        self._color_cat = color_cat\n",
    "        self._price = price\n",
    "        self._dept = dept\n",
    "\n",
    "    # Getter function\n",
    "    def get_name(self) -> str:\n",
    "        return self._name\n",
    "    def get_color(self) -> str:\n",
    "        return self._color\n",
    "    def get_color_cat(self) -> str:\n",
    "        return self._color_cat\n",
    "    def get_price(self) -> int:\n",
    "        return self._price\n",
    "    def get_dept(self) -> str:\n",
    "        return self._dept\n",
    "\n",
    "    # Setter function\n",
    "    def set_name(self, name) -> None:\n",
    "        self._name = name\n",
    "    def set_color(self, color) -> None:\n",
    "        self._color = color\n",
    "    def set_color_cat(self, color_cat) -> None:\n",
    "        self._color_cat = color_cat\n",
    "    def set_price(self, price) -> None:\n",
    "        self._price = price\n",
    "    def set_dept(self, dept) -> None:\n",
    "        self._dept = dept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Build an dictionary to store the webpage which failed to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "failed_page = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Implement an function to scrap a single web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode those departments and colors into url format\n",
    "import urllib\n",
    "\n",
    "# Function of a single web scrapping\n",
    "def single_web_scrap(color_name, dept_name):\n",
    "    print(f\"Working on pairs:{dept_name}:{color_name}\")\n",
    "    product_per_page = 120      # Each page of endclothing.com will shows 120 product cards\n",
    "    null_counter = 0            # Null counter for checking is there any missing item from scrapping\n",
    "    product_index = 1           # Count the number of item that had been scrapped\n",
    "    driver = webdriver.ChromiumEdge()\n",
    "\n",
    "    # Build an try-catch block to do a http request to each webpage\n",
    "    # After having the number of the corresponding department and color\n",
    "    # The page number can act as the url parameter to do another http request for those pages afterward\n",
    "    try:\n",
    "        driver.get(F\"https://www.endclothing.com/ca/clothing?colour={color_name}&department={dept_name}\")\n",
    "        product_count = driver.find_element(By.XPATH, '//div[@data-test-id=\"Product__Counter\"]')\n",
    "        product_count, _ = product_count.text.split(' ')\n",
    "        page_count = (int(product_count)//product_per_page) + 1\n",
    "        page_number = 1\n",
    "        print(page_count)\n",
    "    except Exception as e:\n",
    "            # When the http request having error, store the department name and the color name in to failed_page dict to prepare the another scrap\n",
    "            # Then return the function to free a thread back into the thread pool\n",
    "            print(e)\n",
    "            failed_page[\"dept_name\"].append(urllib.parse.unquote(dept_name))\n",
    "            failed_page[\"color_name\"].append(urllib.parse.unquote(color_name))\n",
    "            failed_page[\"error_message\"].append(e)\n",
    "            driver.close()\n",
    "            return []\n",
    "\n",
    "    product_card_list:[ProductCard] = []        # Build a list of custom build class \"ProductCard\" to store the product cards from scrapping\n",
    "\n",
    "    while page_number < page_count+1:           # Traverse each page of corresponding the department and color\n",
    "        print(f\"Scrapping on page {page_number}\")\n",
    "        driver.get(F\"https://www.endclothing.com/ca/clothing?colour={color_name}&department={dept_name}&page={page_number}\")\n",
    "\n",
    "        # Point to the product card element first and do the scrolling\n",
    "        product_cards = driver.find_elements(By.XPATH, '//a[@data-test-id=\"ProductCard__ProductCardSC\"]')\n",
    "        for i in range(len(product_cards)):\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", product_cards[i])\n",
    "\n",
    "        # Point to element name, color, and price from each product card\n",
    "        temp_product_card_list:[ProductCard] = []\n",
    "        for product_card in product_cards:\n",
    "            name = product_card.find_element(By.XPATH, './/span[@data-test-id=\"ProductCard__PlpName\"]').text\n",
    "            color = product_card.find_element(By.XPATH, './/span[@data-test-id=\"ProductCard__ProductColor\"]').text\n",
    "            price = product_card.find_element(By.XPATH, './/span[@data-test-id=\"ProductCard__ProductFinalPrice\"]').text\n",
    "            print(f\"{product_index}, name: {name}, color: {color}, price: {price}\")\n",
    "            product_index += 1\n",
    "\n",
    "            # Check whether the product card information is empty\n",
    "            # Otherwise, init a ProductCard object and store those information in it\n",
    "            if name == '' or color == '' or price == '':\n",
    "                null_counter += 1\n",
    "            else:\n",
    "                price = int(price[3:].replace(',',''))\n",
    "                product_card = ProductCard(name, color, urllib.parse.unquote(color_name), price, urllib.parse.unquote(dept_name))\n",
    "                print(F\"{product_card.get_name()}, {product_card.get_color()}, {product_card.get_price()}, {product_card.get_dept()}\")\n",
    "                temp_product_card_list.append(product_card)\n",
    "\n",
    "        # If there are missing information from product card, do the web scrapping again\n",
    "        if null_counter > 0:\n",
    "            page_number -= 1\n",
    "            null_counter = 0\n",
    "        else:\n",
    "            print(temp_product_card_list)\n",
    "            product_card_list.extend(temp_product_card_list)\n",
    "        \n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "    print(null_counter)\n",
    "    print(F\"product_card_list: {product_card_list}\")\n",
    "    print(F\"No. of product card: {len(product_card_list)}\")\n",
    "    print(F\"{product_card_list[0].get_name()}, {product_card_list[0].get_color()}, {product_card_list[0].get_price()}, {product_card_list[0].get_dept()}\")\n",
    "    driver.close()\n",
    "    return product_card_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Build a multithreading approach for scrapping each department and color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-thread pool for multiple pages scrapping\n",
    "import concurrent.futures\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "counter = 0\n",
    "future_to_url = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for dept in depts_list:\n",
    "        for color in colors_list:\n",
    "            print(f\"dept:{dept}, color:{color}\")\n",
    "            future_to_url.append(executor.submit(single_web_scrap, urllib.parse.quote_plus(color), urllib.parse.quote(dept)))\n",
    "\n",
    "    data = []\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        try:\n",
    "            data.extend(future.result())\n",
    "        except Exception as exc:\n",
    "            print(exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Check the data after finishing all web scrapping and store those failed pages in scrapping in \"failed_page_list.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in range(0, 10):\n",
    "    product_card:ProductCard = data[i]\n",
    "    print(product_card.get_name(), product_card.get_price(),product_card.get_color(), product_card.get_color_cat(), product_card.get_dept())\n",
    "\n",
    "print(len(data))\n",
    "print(len(failed_page))\n",
    "failed_page_df = pd.DataFrame.from_dict(failed_page)\n",
    "print(failed_page_df)\n",
    "failed_page_df.to_csv(\"failed_page_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Extract those departments and colors from failed page list and do the scrapping again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_color = failed_page[\"color_name\"]\n",
    "missing_dept = failed_page[\"dept_name\"]\n",
    "print(len(missing_color),missing_color)\n",
    "print(len(missing_dept),missing_dept)\n",
    "\n",
    "for i in range(len(missing_color)):\n",
    "    data.extend(single_web_scrap(urllib.parse.quote_plus(missing_color[i]), urllib.parse.quote(missing_dept[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of data web scrapped from endclothing.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32542\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Store the scrapped information into product_card.csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "title = [\"name\", \"color\", \"color category\", \"price\", \"department\"]\n",
    "\n",
    "with open('product_card_4.csv', 'w', newline='') as file:\n",
    "    # Step 4: Using csv.writer to write the list to the CSV file\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(title) # Use writerow for single list\n",
    "    for product_card in data:\n",
    "        writer.writerow([product_card.get_name(), product_card.get_color(), product_card.get_color_cat(), product_card.get_price(), product_card.get_dept()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below shows the structure of the product_card.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32542 entries, 0 to 32541\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   name            32542 non-null  object\n",
      " 1   color           32542 non-null  object\n",
      " 2   color category  32542 non-null  object\n",
      " 3   price           32542 non-null  int64 \n",
      " 4   department      32542 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "end_clothing_product_df = pd.read_csv(\"product_card.csv\")\n",
    "\n",
    "print(end_clothing_product_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
